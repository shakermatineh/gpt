{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/output/path\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/output/path\")\n",
    "\n",
    "# Sample QA data\n",
    "qa_data = [\n",
    "    {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n",
    "    {\"question\": \"Who wrote 'Hamlet'?\", \"answer\": \"William Shakespeare\"},\n",
    "    # Add more question-answer pairs...\n",
    "]\n",
    "\n",
    "# Convert to the format expected for fine-tuning\n",
    "def format_qa(example):\n",
    "    return {\n",
    "        \"input_text\": f\"Question: {example['question']} Answer:\",\n",
    "        \"target_text\": f\" {example['answer']}\"  # Add a space before the answer\n",
    "    }\n",
    "\n",
    "formatted_qa_data = [format_qa(qa) for qa in qa_data]\n",
    "\n",
    "# Load LLaMA model and tokenizer (replace with your model path)\n",
    "tokenizer = LLaMATokenizer.from_pretrained(\"path/to/llama-model\")\n",
    "model = LLaMAForCausalLM.from_pretrained(\"path/to/llama-model\")\n",
    "\n",
    "# Prepare dataset and tokenize\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples['input_text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    targets = tokenizer(examples['target_text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    inputs['labels'] = targets['input_ids']\n",
    "    return inputs\n",
    "\n",
    "# Create Hugging Face dataset\n",
    "dataset = Dataset.from_list(formatted_qa_data)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama-finetuned-qa\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
