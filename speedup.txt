By default in pytorch when creating tensors (activations, parameters) everything is in float32.
This is too much in deep learning training. Nvidia A100 supports even FP64 but we don't need for DL. 
Based on A100 specs for FP32 we expect 19.5 tera flops which mean 19.5 trillion FP operations per second as theoretical peak performance.

If we go lower precision to TF32 we get 8x gain and if we go even lower to FP16 Tensor Core and BFLOAT16 we get 16x gain.
INT8 Tensor Core offers 32x gain, but that is used for inference not training. It has uniform spacing, but we want FP matches better match to normal disto of weights and activations during training NNs.

If numbers have fewer bits it's easier to move them arround. This is where we talk about memory bandwidth.
We have finite capacity on number of bits GPU can store, but there's a speed to access that memory. We have certain memory bandwidth which is precious resource.

Most of DL workloads for training are memory bound. The tensor cores doing fast muliplication most of the time are idle, we can't feed them with data fast enough. We can load fast enough from memory.
If we get 60% utilization we're doing very well, means half of time data is not available to tensorcores.


Tensor Core
It is an instruction in A100, does 4-by-4 matrix mult.
TF32 has exact 8 bits for exponent of floating point, but it mantissa bits get cropped in TF32, instead of 23 bits of FP32.
TF32 accumlator (intermediate +=) happens in FP32.
All of these are internal to instruction, in pytroch level all numbers look identical. 
When we call TC instruction internally in hardward crops out 13 bits of mantissa. That makes the 4-by-4 matrix mult 8x faster, and a bit more approximate.

As CPU runs, it schedules work on GPU. Sends request and continues running. If we measure time using time.time() CPU does that work.
It could happen that GPU is still working and CPU measures time it took to run some code inaccurately.
We need to do torch.cuda.synchronise() and that waits for the GPU to finish all the scheduled runs, and then we can take the time.

For BFLOAT16 mantissa is even more cropped to 7 bits, and exponent is keps at 8 bits.
In FP16 has a reduce range because it crops the exponent to 5 bits. That's problematics, since we need to do gradient scaling.
So BFLOAT16 is preferred, but we do change the tensors as we see them in pytorch.

Only helpful resource in Automatic Mixed Precision in pytorch:
https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html#adding-torch-autocast
Contex manager that we want to run forward pass under autocast is:
`with torch.autocast(device_type=device, dtype=torch.float16):`
Clicking on torch.autocast there are more useful guidelines.
Do not call half() or bfloat16() on any tensors or input, just use autocast. 
Only surround forward pass and loss calculation and leave the backward and optimzier alone!

Some ops are converted to bfloat16 some aren't.
https://pytorch.org/docs/stable/amp.html#cuda-ops-that-can-autocast-to-float16
matrix mult are more robust to reduced precision.
Operations like normalizations, layernorm, softmax, log, loss functions remain in tf32 because they're more susceptible to precision changes.

torch.compile is a compiler for NNs. It takes out python interpreter.
torch.compile analyzes the whole code and knows what operations it has to run and optimizes.
Without that python interpreter runs in eager mode, starts at forward, goes layer by layer, materialized operations as it goes through.
Also 