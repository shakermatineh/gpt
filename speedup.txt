By default in pytorch when creating tensors (activations, parameters) everything is in float32.
This is too much in deep learning training. Nvidia A100 supports even FP64 but we don't need for DL. 
Based on A100 specs for FP32 we expect 19.5 tera flops which mean 19.5 trillion FP operations per second as theoretical peak performance.

If we go lower precision to TF32 we get 8x gain and if we go even lower to FP16 Tensor Core and BFLOAT16 we get 16x gain.
INT8 Tensor Core offers 32x gain, but that is used for inference not training. It has uniform spacing, but we want FP matches better match to normal disto of weights and activations during training NNs.

If numbers have fewer bits it's easier to move them arround. This is where we talk about memory bandwidth.
We have finite capacity on number of bits GPU can store, but there's a speed to access that memory. We have certain memory bandwidth which is precious resource.

Most of DL workloads for training are memory bound. The tensor cores doing fast muliplication most of the time are idle, we can't feed them with data fast enough. We can load fast enough from memory.
If we get 60% utilization we're doing very well, means half of time data is not available to tensorcores.


Tensor Core
It is an instruction in A100, does 4-by-4 matrix mult.
TF32 has exact 8 bits for exponent of floating point, but it mantissa bits get cropped in TF32, instead of 23 bits of FP32.
TF32 accumlator (intermediate +=) happens in FP32.
All of these are internal to instruction, in pytroch level all numbers look identical. 
When we call TC instruction internally in hardward crops out 13 bits of mantissa. That makes the 4-by-4 matrix mult 8x faster, and a bit more approximate.

As CPU runs, it schedules work on GPU. Sends request and continues running. If we measure time using time.time() CPU does that work.
It could happen that GPU is still working and CPU measures time it took to run some code inaccurately.
We need to do torch.cuda.synchronise() and that waits for the GPU to finish all the scheduled runs, and then we can take the time.

For BFLOAT16 mantissa is even more cropped to 7 bits, and exponent is keps at 8 bits.
In FP16 has a reduce range because it crops the exponent to 5 bits. That's problematics, since we need to do gradient scaling.
So BFLOAT16 is preferred, but we do change the tensors as we see them in pytorch.

Only helpful resource in Automatic Mixed Precision in pytorch:
https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html#adding-torch-autocast
Contex manager that we want to run forward pass under autocast is:
`with torch.autocast(device_type=device, dtype=torch.float16):`
Clicking on torch.autocast there are more useful guidelines.
Do not call half() or bfloat16() on any tensors or input, just use autocast. 
Only surround forward pass and loss calculation and leave the backward and optimzier alone!

Some ops are converted to bfloat16 some aren't.
https://pytorch.org/docs/stable/amp.html#cuda-ops-that-can-autocast-to-float16
matrix mult are more robust to reduced precision.
Operations like normalizations, layernorm, softmax, log, loss functions remain in tf32 because they're more susceptible to precision changes.

torch.compile is a compiler for NNs. It takes out python interpreter.
torch.compile analyzes the whole code and knows what operations it has to run and optimizes.
Without that python interpreter runs in eager mode, starts at forward, goes layer by layer, materialized operations as it goes through.
Gain come from reducing python overhead and GPU read/writes.

There are operations that torch.compile will not find like flash attention.
Flash attention does more flops than regular attention implementation, but it's significantly faster.
It's very mindful of memory hierarchy, what's in high bandwidth memeory and shared memory. 
It's careful about orchestrating the computation in a way that we have fewer reads and write from and to hight bandwidth memory.
The expensive part is load and store from and to high bandwidth memory.
Flash attention is designed in a way that the big attention matrix (att) never gets materialized.
That's the matrix that queries and keys interact. For each head for each batch element we get a T*T matrix, which is millions numbers.
It relies on online softmax trick shows how to increamentally evaluate softmax without having to realize all inputs in softmax during normalization.
[Online normalizer calculation for softmax] from nvidia. Streaming softmax!
Flops don't matter, memory access pattern matters. Torch compile is amazing, but there are many optimization torch.compile can't find.

Everything in cuda works with nice numbers which means powers of two, lots of kernels are written in powers of twos.
Ugly numbers makes optimizing kernels very hard. For instance 50257 is a very ugly number!

We use details from gpt3 paper in gpt2 training, paper gives a lot more detials,
but they never released gpt3 model itself. The architecure very similar, context length 
increase to 2048 from 1024 and some hyperparameters in transformers.
Cosine lr has been popularized in gpt2/3

Distributed Data Parallel
https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html

we have 8 gpus, we're going to launch 8 processes and assign to each gpu.
They're going to be processing different parts of data. Once they all calculate we average those gradients.
torchrun runs all 8, creates env variables where each processes runs based on them.

Datasets reprentative of what gpt2/3 were trained on.
clean and depuped subset of RedPajama dataset:
https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama
Another one is Fineweb, a high quality common crawl. https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1
HF released a subset of it is fineweb-edu, high quality eduational subset. 
We tain on sample-10BT subset. That's enough to get close enough to gpt2 performance.

tokens in terms of lengths:
1 token ~= 4 chars in English. 1 token ~= Â¾ words. 100 tokens ~= 75 words.

Benchmark Eval dataset Hellaswag: https://arxiv.org/pdf/1905.07830
4-choice sentence completion.
answers are generated by LMs. They were easy for human but difficult for LMs in 2019, now solved.
GPT3 still used Hellaswag. It's popular since it it smooth, gives early signals even with small models.
For small models, the way we feed the Hellaswag dataset is to create a batch of 4 rows, one for each answer.
First the context is added to all rows, then each answer is appended, probabilites of generated tokens for each are averaged and largest is picked.
Larger models can accept all 4 answers, this has the advantage that model see all 4 before giving an answer.

