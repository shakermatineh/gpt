By default in pytorch when creating tensors (activations, parameters) everything is in float32.
This is too much in deep learning training. Nvidia A100 supports even FP64 but we don't need for DL. 
Based on A100 specs for FP32 we expect 19.5 tera flops which mean 19.5 trillion FP operations per second as theoretical peak performance.

If we go lower precision to TF32 we get 8x gain and if we go even lower to FP16 Tensor Core and BFLOAT16 we get 16x gain.
INT8 Tensor Core offers 32x gain, but that is used for inference not training. It has uniform spacing, but we want FP matches better match to normal disto of weights and activations during training NNs.

If numbers have fewer bits it's easier to move them arround. This is where we talk about memory bandwidth.
We have finite capacity on number of bits GPU can store, but there's a speed to access that memory. We have certain memory bandwidth which is precious resource.

Most of DL workloads for training are memory bound. The tensor cores doing fast muliplication most of the time are idle, we can't feed them with data fast enough. We can load fast enough from memory.
If we get 60% utilization we're doing very well, means half of time data is not available to tensorcores.


Tensor Core
It is an instruction in A100, does 4-by-4 matrix mult.
TF32 has exact 8 bits for exponent of floating point, but it mantissa bits get cropped in TF32, instead of 23 bits of FP32.
TF32 accumlator (intermediate +=) happens in FP32.
All of these are internal to instruction, in pytroch level all numbers look identical. 
When we call TC instruction internally in hardward crops out 13 bits of mantissa. That makes the 4-by-4 matrix mult 8x faster, and a bit more approximate.

As CPU runs, it schedules work on GPU. Sends request and continues running. If we measure time using time.time() CPU does that work.
It could happen that GPU is still working and CPU measures time it took to run some code inaccurately.
We need to do torch.cuda.synchronise() and that waits for the GPU to finish all the scheduled runs, and then we can take the time.

For BFLOAT16 mantissa is even more cropped to 7 bits, and exponent is keps at 8 bits.
In FP16 has a reduce range because it crops the exponent to 5 bits. That's problematics, since we need to do gradient scaling.
So BFLOAT16 is preferred, but we do change the tensors as we see them in pytorch.

Only helpful resource in Automatic Mixed Precision in pytorch:
https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html#adding-torch-autocast
Contex manager that we want to run forward pass under autocast is:
`with torch.autocast(device_type=device, dtype=torch.float16):`
Clicking on torch.autocast there are more useful guidelines.
Do not call half() or bfloat16() on any tensors or input, just use autocast. 
Only surround forward pass and loss calculation and leave the backward and optimzier alone!

Some ops are converted to bfloat16 some aren't.
https://pytorch.org/docs/stable/amp.html#cuda-ops-that-can-autocast-to-float16
matrix mult are more robust to reduced precision.
Operations like normalizations, layernorm, softmax, log, loss functions remain in tf32 because they're more susceptible to precision changes.

torch.compile is a compiler for NNs. It takes out python interpreter.
torch.compile analyzes the whole code and knows what operations it has to run and optimizes.
Without that python interpreter runs in eager mode, starts at forward, goes layer by layer, materialized operations as it goes through.
Gain come from reducing python overhead and GPU read/writes.

There are operations that torch.compile will not find like flash attention.
Flash attention does more flops than regular attention implementation, but it's significantly faster.
It's very mindful of memory hierarchy, what's in high bandwidth memeory and shared memory. 
It's careful about orchestrating the computation in a way that we have fewer reads and write from and to hight bandwidth memory.
The expensive part is load and store from and to high bandwidth memory.
Flash attention is designed in a way that the big attention matrix (att) never gets materialized.
That's the matrix that queries and keys interact. For each head for each batch element we get a T*T matrix, which is millions numbers.
It relies on online softmax trick shows how to increamentally evaluate softmax without having to realize all inputs in softmax during normalization.
[Online normalizer calculation for softmax] from nvidia. Streaming softmax!
Flops don't matter, memory access pattern matters. Torch compile is amazing, but there are many optimization torch.compile can't find.

Everything in cuda works with nice numbers which means powers of two, lots of kernels are written in powers of twos.
Ugly numbers makes optimizing kernels very hard. For instance 50257 is a very ugly number!

We use details from gpt3 paper in gpt2 training, paper gives a lot more detials,
but they never released gpt3 model itself. The architecure very similar, context length 
increase to 2048 from 1024 and some hyperparameters in transformers.
Cosine lr has been popularized in gpt2/3

Distributed Data Parallel
https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html

we have 8 gpus, we're going to launch 8 processes and assign to each gpu.
They're going to be processing different parts of data. Once they all calculate we average those gradients.
torchrun runs all 8, creates env variables where each processes runs based on them.

Datasets reprentative of what gpt2/3 were trained on.
clean and depuped subset of RedPajama dataset:
https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama
Another one is Fineweb, a high quality common crawl. https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1
HF released a subset of it is fineweb-edu, high quality eduational subset. 
We tain on sample-10BT subset. That's enough to get close enough to gpt2 performance.

tokens in terms of lengths:
1 token ~= 4 chars in English. 1 token ~= Â¾ words. 100 tokens ~= 75 words.

Benchmark Eval dataset Hellaswag: https://arxiv.org/pdf/1905.07830
4-choice sentence completion.
answers are generated by LMs. They were easy for human but difficult for LMs in 2019, now solved.
GPT3 still used Hellaswag. It's popular since it it smooth, gives early signals even with small models.
For small models, the way we feed the Hellaswag dataset is to create a batch of 4 rows, one for each answer.
First the context is added to all rows, then each answer is appended, probabilites of generated tokens for each are averaged and largest is picked.
Larger models can accept all 4 answers, this has the advantage that model see all 4 before giving an answer.

This is another evaluatin dataset for LMs:
https://github.com/EleutherAI/lm-evaluation-harness

If we want to talk to the model, we need to FT for chat.
A conversational dataset with assistant user sections.
We fill in the user tokens and sample the assistant tokens.
We swap the dataset and continue training.

Tokenizer
https://tiktokenizer.vercel.app

cl100k_base: gpt4 tokenizer, half as many tokens in a sequence compared to gpt2.
disadvantage is tha embedding table and softmax table grows too. there is a soft spot.

Python docs: "Strings are immutable sequences of unicode codepoints."

What is Unicode?
Unicode is a universal character encoding standard that assigns a unique number (code point) 
to every character from every writing system, as well as symbols, punctuation marks, and emojis. 
This standard ensures that text looks the same on different systems and platforms.

What is code point?
An integer that corresponds to a specific character in a character encoding standard, such as Unicode.

wikipedia unicode: 149813 characters right now, latest update 2023
The way to access unicode code point for a character is by using ord() in python.
ord("h") = 104. For single character "h" unicode code point is 104.
ord("ðŸ‘‹") unicode code point is 128075. 

The raw code points are already assigned integers.
Why can't we just use these integers and not having tokenizer? vocab would be too long. 150k.
More importantly unice standard is very much alive and still changing. not stable representation.
So we need something better, so we turn into encodings.

What is Encoding?
An encoding is a way to convert these Unicode code points into a sequence of bytes 
that can be stored in memory, transmitted over networks, or processed by computers.
Encodings define how the abstract Unicode code points are represented as a series of bits.

From Unicode wiki: "The Unicode Standard defines three encodings: UTF-8, UTF-16, and UTF-32,"
These are the ways we can take unicode text and translate it to binary data or byte streams.
utf-8 is by far the most common. It takes every code point and transfers it to byte streams.
This byte stream of UTF-8 is between 1 to 4 bytes, so it's variable length encoding.
UTF-32 on the other hand is fixed-length. Each encoding has its own pros and cons. UTF-8
is preferred, one reason is that it is backward compatible with ASCII.
blog: utf-8 everywhere manifesto.

The first 128 characters (0-127) are identical to ASCII, making it compatible with ASCII text.
There are 128 Ascii characters, represnted by one byte. More complex characters are represented by more bytes.
To get bytes from strings we do "ðŸ‘‹".encode("utf-8"). This gives a sequence of 4 bytes: b'\xf0\x9f\x91\x8b'.
Each \xNN is a hexadecimal representation. Two numbers each range from 1 to 9, A, B, C, D, E, F. So there
are 16 * 16 = 256 of these representations. Each is a byte Id. list("ðŸ‘‹".encode("utf-8")) = [240, 159, 145, 139] shows
sequence of 4 bytes representing the hand-wave emoji.

We can in theory use 256 raw byte IDs of utf-8 encoding for tokenizer but that implies vocab_size of 256, 
too small, makes sequences too long and attention becomes expensive. we want to stick to utf-8 but have 
larger vocab size. That is BPE. First raw byte sequences are produces by then they are merged and compressed.
There's research on feeding raw byte sequence streams to transformers.
That is tokenization-free autoregressive sequence modeling at scale. 

BPE: iterativley find pairs of tokens that occur most frequently, we replace that with a single new token
we replacce every occourance with new token. Example in BPE wiki. tokenizer has its own training set, 
can be different from llm training dataset. Tokenizer is trained using BPE.

Merging hueristics on top of BPE.
Below is just the inference code for tokenizer, not training.
https://github.com/openai/gpt-2/blob/master/src/encoder.py#L53
We don't know the exat code for training tokenizer, but it's not as simple as chunk it up then BPE it. 
