By default in pytorch when creating tensors (activations, parameters) everything is in float32.
This is too much in deep learning training. Nvidia A100 supports even FP64 but we don't need for DL. 
Based on A100 specs for FP32 we expect 19.5 tera flops which mean 19.5 trillion FP operations per second as theoretical peak performance.

If we go lower precision to TF32 we get 8x gain and if we go even lower to FP16 Tensor Core and BFLOAT16 we get 16x gain.
INT8 Tensor Core offers 32x gain, but that is used for inference not training. It has uniform spacing, but we want FP matches better match to normal disto of weights and activations during training NNs.

If numbers have fewer bits it's easier to move them arround. This is where we talk about memory bandwidth.
We have finite capacity on number of bits GPU can store, but there's a speed to access that memory. We have certain memory bandwidth which is precious resource.

Most of DL workloads for training are memory bound. The tensor cores doing fast muliplication most of the time are idle, we can't feed them with data fast enough. We can load fast enough from memory.
If we get 60% utilization we're doing very well, means half of time data is not available to tensorcores.


Tensor Core
It is an instruction in A100, does 4-by-4 matrix mult.
TF32 has exact 8 bits for exponent of floating point, but it mantissa bits get cropped in TF32, instead of 23 bits of FP32.
TF32 accumlator (intermediate +=) happens in FP32.
All of these are internal to instruction, in pytroch level all numbers look identical. 
When we call TC instruction internally in hardward crops out 13 bits of mantissa. That makes the 4-by-4 matrix mult 8x faster, and a bit more approximate.

As CPU runs, it schedules work on GPU. Sends request and continues running. If we measure time using time.time() CPU does that work.
It could happen that GPU is still working and CPU measures time it took to run some code inaccurately.
We need to do torch.cuda.synchronise() and that waits for the GPU to finish all the scheduled runs, and then we can take the time.

For BFLOAT16 mantissa is even more cropped to 7 bits, and exponent is keps at 8 bits.
In FP16 has a reduce range because it crops the exponent to 5 bits. That's problematics, since we need to do gradient scaling.
So BFLOAT16 is preferred, but we do change the tensors as we see them in pytorch.

Only helpful resource in Automatic Mixed Precision in pytorch:
https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html#adding-torch-autocast
Contex manager that we want to run forward pass under autocast is:
`with torch.autocast(device_type=device, dtype=torch.float16):`
Clicking on torch.autocast there are more useful guidelines.
Do not call half() or bfloat16() on any tensors or input, just use autocast. 
Only surround forward pass and loss calculation and leave the backward and optimzier alone!

Some ops are converted to bfloat16 some aren't.
https://pytorch.org/docs/stable/amp.html#cuda-ops-that-can-autocast-to-float16
matrix mult are more robust to reduced precision.
Operations like normalizations, layernorm, softmax, log, loss functions remain in tf32 because they're more susceptible to precision changes.

torch.compile is a compiler for NNs. It takes out python interpreter.
torch.compile analyzes the whole code and knows what operations it has to run and optimizes.
Without that python interpreter runs in eager mode, starts at forward, goes layer by layer, materialized operations as it goes through.
Gain come from reducing python overhead and GPU read/writes.

There are operations that torch.compile will not find like flash attention.
Flash attention does more flops than regular attention implementation, but it's significantly faster.
It's very mindful of memory hierarchy, what's in high bandwidth memeory and shared memory. 
It's careful about orchestrating the computation in a way that we have fewer reads and write from and to hight bandwidth memory.
The expensive part is load and store from and to high bandwidth memory.
Flash attention is designed in a way that the big attention matrix (att) never gets materialized.
That's the matrix that queries and keys interact. For each head for each batch element we get a T*T matrix, which is millions numbers.
It relies on online softmax trick shows how to increamentally evaluate softmax without having to realize all inputs in softmax during normalization.
[Online normalizer calculation for softmax] from nvidia. Streaming softmax!
Flops don't matter, memory access pattern matters. Torch compile is amazing, but there are many optimization torch.compile can't find.

Everything in cuda works with nice numbers which means powers of two, lots of kernels are written in powers of twos.
Ugly numbers makes optimizing kernels very hard. For instance 50257 is a very ugly number!

Hyperparameter:
We use details from gpt3 paper in gpt2 training, paper gives a lot more detials,
but they never released gpt3 model itself. The architecure very similar, context length 
increase to 2048 from 1024 and some hyperparameters in transformers.
Cosine lr has been popularized in gpt2/3.

Gradual batch size increase:
gradients initially are correlated. The all learn simple stuff about biases
that what tokens aprea and what not. So we can get same gradient from 32 samples that we get from 1024. 
So there's no need to have high batch size initially. As we progress in training it gets harder and gradients
become decorrelated, so we need higher sample size to statistical power.

Weight decay of 0.1 for small regularizaton effect. 10 times higher than default optimizer.
It is common to not weight decay one-dim tensors like layernorm, scales and biases.
We weight decay embeddings and matmul weights. We do not allow any one of the weights to be too large.

Distributed Data Parallel
https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html

we have 8 gpus, we're going to launch 8 processes and assign to each gpu.
They're going to be processing different parts of data. Once they all calculate we average those gradients.
torchrun runs all 8, creates env variables where each processes runs based on them.

Datasets reprentative of what gpt2/3 were trained on.
clean and depuped subset of RedPajama dataset:
https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama
Another one is Fineweb, a high quality common crawl. https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1
HF released a subset of it is fineweb-edu, high quality eduational subset. 
We tain on sample-10BT subset. That's enough to get close enough to gpt2 performance.

tokens in terms of lengths:
1 token ~= 4 chars in English. 1 token ~= Â¾ words. 100 tokens ~= 75 words.

Benchmark Eval dataset Hellaswag: https://arxiv.org/pdf/1905.07830
4-choice sentence completion.
answers are generated by LMs. They were easy for human but difficult for LMs in 2019, now solved.
GPT3 still used Hellaswag. It's popular since it it smooth, gives early signals even with small models.
For small models, the way we feed the Hellaswag dataset is to create a batch of 4 rows, one for each answer.
First the context is added to all rows, then each answer is appended, probabilites of generated tokens for each are averaged and largest is picked.
Larger models can accept all 4 answers, this has the advantage that model see all 4 before giving an answer.

This is another evaluatin dataset for LMs:
https://github.com/EleutherAI/lm-evaluation-harness

If we want to talk to the model, we need to FT for chat.
A conversational dataset with assistant user sections.
We fill in the user tokens and sample the assistant tokens.
We swap the dataset and continue training.

Tokenizer
https://tiktokenizer.vercel.app

cl100k_base: gpt4 tokenizer, half as many tokens in a sequence compared to gpt2.
disadvantage is tha embedding table and softmax table grows too. there is a soft spot.

Python docs: "Strings are immutable sequences of unicode codepoints."

What is Unicode?
Unicode is a universal character encoding standard that assigns a unique number (code point) 
to every character from every writing system, as well as symbols, punctuation marks, and emojis. 
This standard ensures that text looks the same on different systems and platforms.

What is code point?
An integer that corresponds to a specific character in a character encoding standard, such as Unicode.

wikipedia unicode: 149813 characters right now, latest update 2023
The way to access unicode code point for a character is by using ord() in python.
ord("h") = 104. For single character "h" unicode code point is 104.
ord("ðŸ‘‹") unicode code point is 128075. 

The raw code points are already assigned integers.
Why can't we just use these integers and not having tokenizer? vocab would be too long. 150k.
More importantly unice standard is very much alive and still changing. not stable representation.
So we need something better, so we turn into encodings.

What is Encoding?
An encoding is a way to convert these Unicode code points into a sequence of bytes 
that can be stored in memory, transmitted over networks, or processed by computers.
Encodings define how the abstract Unicode code points are represented as a series of bits.

From Unicode wiki: "The Unicode Standard defines three encodings: UTF-8, UTF-16, and UTF-32,"
These are the ways we can take unicode text and translate it to binary data or byte streams.
utf-8 is by far the most common. It takes every code point and transfers it to byte streams.
This byte stream of UTF-8 is between 1 to 4 bytes, so it's variable length encoding.
UTF-32 on the other hand is fixed-length. Each encoding has its own pros and cons. UTF-8
is preferred, one reason is that it is backward compatible with ASCII.
blog: utf-8 everywhere manifesto.

The first 128 characters (0-127) are identical to ASCII, making it compatible with ASCII text.
There are 128 Ascii characters, represnted by one byte. More complex characters are represented by more bytes.
To get bytes from strings we do "ðŸ‘‹".encode("utf-8"). This gives a sequence of 4 bytes: b'\xf0\x9f\x91\x8b'.
Each \xNN is a hexadecimal representation. Two numbers each range from 1 to 9, A, B, C, D, E, F. So there
are 16 * 16 = 256 of these representations. Each is a byte Id. list("ðŸ‘‹".encode("utf-8")) = [240, 159, 145, 139] shows
sequence of 4 bytes representing the hand-wave emoji.

We can in theory use 256 raw byte IDs of utf-8 encoding for tokenizer but that implies vocab_size of 256, 
too small, makes sequences too long and attention becomes expensive. we want to stick to utf-8 but have 
larger vocab size. That is BPE. First raw byte sequences are produces by then they are merged and compressed.
There's research on feeding raw byte sequence streams to transformers.
That is tokenization-free autoregressive sequence modeling at scale. 

BPE: iterativley find pairs of tokens that occur most frequently, we replace that with a single new token
we replacce every occourance with new token. Example in BPE wiki. tokenizer has its own training set, 
can be different from llm training dataset. Tokenizer is trained using BPE.

Merging hueristics on top of BPE.
Below is just the inference code for tokenizer, not training.
https://github.com/openai/gpt-2/blob/master/src/encoder.py#L53
We don't know the exat code for training tokenizer, but it's not as simple as chunk it up then BPE it. 

Special tokens.
gpt2 has one special token <|endoftext|>. Model should learn things coming after this are not related anymore.
special tokens are important specially in finetuned and chat models
registering special token in gpt2: https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py#L25
gpt4 has 5 special tokens: https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py#L87
It has FIM tokens, fill in the middle: https://arxiv.org/pdf/2207.14255
we can extend tiktokens adding new special tokens. instructions in tiktoken github.
When adding special tokens, we have to do model surgery in transformer. 
Need to makes sure embedding matrix extended, initialized random.
Also final layer and projection layer has to be extended.
Common operation specially in finetuning, extending from base model to chat model.
Tiktoken code can only do inference, we need to write our own code for training tokenizer.

sentencepiece vs tiktoken
Unlike tiktoken, we can do both training and inference.
It is used by both llama and mistral series. Compared to tikton the order of operations if different
Tiktoken First takes code point, encode then using utf-8 to bytes and merge bytes.
In sentencepiece BPE is done on code points, and falls back to bytes for rare code points. 
Rare codepoints are specified with hyperparameter character_coverage, they are either mapped to UNK token, 
or if byte_fallback is on, the rare code points are encoded using utf-8, and bytes will be transformed 
to tokens and special byte tokens are added to vocabulary.

How to define vocab_size?

How to take pretrained and extend vocab size for special functionality?
finetunign for chatgpt, new special tokens on top of base model to maintain the metadata and structure
of conversation between user and system. Also we may need special tokens for using browser or any other tool.
Resize embeddings, initialize new random small numbers. Freeze base model, introduce new parameters and only
train new parameters to introduce new tokens into the architecure. Freeze/train arbitrary parts of it.

introducing new vocabulary goes beyond just adding special tokens. There's an entire design space.
Paper: learning to compress prompts with gist tokens. https://arxiv.org/pdf/2304.08467
If prompt is very long things slow down. instead we can introduce new tokens put them in sequence, trian model
by distillation, rest frozen, we train such that behavior is identical to model with very long prompt.
At test time we can use new tokens and no need to have long prompt. It's a technique in PEFT class.

gpt3.5-turbo-instruct is a completion model, not a chat model. A lot closer to base model.

SolidGoldMagikarp token:
https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation
The token appears only in tokenzation training data, but not actual language modeling.
It never occurs and it has a completely untrained vector and invokes undefined behavior 
because it is out of distribution.

Yaml preferred over json because same text represented with fewer tokens. in the token economy
where we pay per context length we need to think about token efficiencies.

If we can use gpt4 vocab in tiktoken that's best, for inference (byte level BPE).
If for some reason we need to train our own tokenizer try BPE with sentencepiece.
But algorithm in inferior to tiktoken.
Another option if need to train tokenizer is to wait for minBPE to become complete.
What we need is tiktoken but with training code. minBPE is an implementation of it in python. 

